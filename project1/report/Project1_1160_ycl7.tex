%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{graphicx} %% inserted
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

% Content lightly modified from original work by Jesse Dodge and Noah Smith


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Technical Report of CS598 Project 1 - Predict the Housing Prices in Ames}

\author{Yu-Chung Lee \\
  \texttt{\{ycl7@illinois.edu\}} \\
  UID: 665451160 \\
  Net ID: ycl7 \\
  } 
\begin{document}
\maketitle

\section{Technical Details}

\subsection{Data Loading and Initial Preparation}
The dataset was organized into training and testing sets, each stored in
separate CSV files within designated fold directories. The training data was
loaded from \texttt{train.csv}, where the first column (PID) was excluded, and
the target variable was extracted from the last column.

The target variable underwent a logarithmic transformation using the
\texttt{log1p} function to stabilize variance and normalize the distribution.
Sale price with extreme values were removed by identifying rows with price
larger or smaller than 3 standard deviations from the mean.

\subsection{Data Cleaning}
\subsubsection{Numerical Features}
\textbf{Garage Year Built (\texttt{Garage\_Yr\_Blt}):} Values exceeding the maximum year of 2011 were considered corrupted and set to \texttt{NaN}. Missing values in \texttt{Garage\_Yr\_Blt} were subsequently imputed using the \texttt{Year\_Built} feature to ensure consistency.

\subsubsection{Categorical Features}
Features with missing values were identified by calculating the sum of nulls in
each column. Any categorical feature containing missing values was excluded
from the analysis by dropping these columns entirely. This step ensured that
the dataset used for modeling did not contain incomplete categorical
information.

\subsubsection{Imbalanced and Irrelevant Variables}
As suggested by the instructor, the following variables were removed from the
dataset: \texttt{Street, Utilities, Condition\_2, Roof\_Matl, Heating,
  Pool\_QC, Misc\_Val, Low\_Qual\_Fin\_SF, Pool\_Area, Longitude, Latitude}.
These variables were consided imbalanced categorical variables with most
samples belonging to a single category or don't offer interpretable
information.

\subsection{Feature Transformation}
\subsubsection{Numerical Features}
Numerical features were identified by checking the data type of each column.
They were further processed with winsorization, skewness transformation and
outlier removal.

\paragraph{Winsorization}
For certain numerical variables, values exceeding the 95th percentile were
capped at the 95th percentile. These variables include \texttt{Lot\_Frontage,
  Lot\_Area, Mas\_Vnr\_Area, BsmtFin\_SF\_2, Bsmt\_Unf\_SF, Total\_Bsmt\_SF,
  Second\_Flr\_SF, First\_Flr\_SF, Gr\_Liv\_Area, Garage\_Area, Wood\_Deck\_SF,
  Open\_Porch\_SF, Enclosed\_Porch, Three\_season\_porch, Screen\_Porch ,and
  Misc\_Val}.

\paragraph{Skewness Transformation}
The skewness of each numerical feature was assessed using the \texttt{skew}
function from the \texttt{scipy.stats} module. Features exhibiting skewness
greater than a threshold of 0.5 were identified as significantly skewed. These
skewed features underwent a logarithmic transformation (\texttt{log1p}) to
reduce skewness and approximate a normal distribution. Features with such
transformations were identified as \texttt{skewed\_feats} and will perform the
same logarithmic transformation on testing set.

\paragraph{Outlier Removal}
For all numerical features, outliers were detected using the Z-score method.
Data points with an absolute Z-score exceeding a threshold of 5 were considered
outliers and subsequently removed from both the feature set and the target
variable. This process was iteratively applied to all numerical features to
enhance the robustness of the models.

\subsubsection{Categorical Encoding}
Categorical variables were transformed into numerical representations using
one-hot encoding via the \texttt{pd.get\_dummies} function. To ensure
consistency between training and testing datasets, the testing set was
reindexed to match the columns of the training set, filling any missing
categories with zeros.

\subsection{Model Implementation}
Multiple regression models were employed to predict the target variable, each
utilizing different algorithms and hyperparameters:

\begin{enumerate}

  \item \textbf{Ridge Regression (\texttt{RidgeCV}):}
        \begin{itemize}
          \item Utilized 5-fold cross-validated Ridge regression with a pipeline that included
                \texttt{RobustScaler} for feature scaling.
          \item A range of alpha values was explored using \texttt{np.logspace(-1, 3, 100)} to
                identify the optimal regularization strength.
          \item Used negative root mean squared error as the scoring metric.
        \end{itemize}

  \item \textbf{XGBoost Regressor (\texttt{XGBRegressor}):}
        \begin{itemize}
          \item Configured with predefined hyperparameters, including \texttt{max\_depth},
                \texttt{learning\_rate}, \texttt{n\_estimators}, and regularization terms
                (\texttt{reg\_alpha}, \texttt{reg\_lambda}).
        \end{itemize}
\end{enumerate}

\subsubsection{Hyperparameter Tuning}
An Optuna-based hyperparameter optimization framework was set up to fine-tune
the \texttt{XGBRegressor} parameters.

\begin{itemize}
  \item The \texttt{objective} function defined the search space for parameters such as
        \texttt{max\_depth}, \texttt{learning\_rate}, \texttt{n\_estimators},
        \texttt{min\_child\_weight}, \texttt{subsample}, \texttt{colsample\_bytree},
        \texttt{reg\_alpha}, and \texttt{reg\_lambda}.
  \item The optimization aimed to minimize RMSE using 5-fold cross-validation over 50
        trials. The best parameters identified from this search is applied to retrain
        the XGBoost model.
\end{itemize}

\subsection{Execution Workflow}
For each of the 10 predefined folds:
\begin{enumerate}
  \item \textbf{Preprocessing training data:} ONLY train datasets for the respective fold were loaded. Then applied the cleaning, transformation, and encoding steps as detailed above.
  \item \textbf{Model Training:} Each regression model was trained on the processed training data.
  \item \textbf{Preprocessing test data:} Test datasets for the respective fold were loaded. Then applied the same preprocessing steps as training data.
  \item \textbf{Prediction:} Predictions on the test dataset were generated and saved into \texttt{mysubmission1.csv} and \texttt{mysubmission2.csv} for Ridge and XGBoost models, respectively.
\end{enumerate}

\section{Performance Metrics}
Models were evaluated with Root-Mean-Squared-Error (RMSE) between the natural
logarithm of the predicted price and the natural logarithm of the observed
sales price. The following table summarizes the RMSE achieved by each model
across all 10 training/test splits. Both the Ridge regression model and XGBoost
model arhived the desired RMSE: 0.125 for the initial 5 training/test splits
and 0.135 for the subsequent 5 training/test splits.

\begin{table}[h]
  \centering
  \begin{tabular}{c c c}
    \Xhline{2\arrayrulewidth}
    RMSE   & Ridge    & XGBoost  \\
    \hline
    fold1  & 0.116114 & 0.111148 \\
    fold2  & 0.117437 & 0.114520 \\
    fold3  & 0.115723 & 0.110675 \\
    fold4  & 0.115164 & 0.115495 \\
    fold5  & 0.108005 & 0.104979 \\
    fold6  & 0.128512 & 0.123666 \\
    fold7  & 0.130906 & 0.129087 \\
    fold8  & 0.129505 & 0.126017 \\
    fold9  & 0.130229 & 0.130740 \\
    fold10 & 0.122325 & 0.122325 \\
    \Xhline{2\arrayrulewidth}
  \end{tabular}
  \caption{RMSE of each model across 10 folds}
\end{table}

\subsection{Hardware Used}
\begin{itemize}
  \item CPU: AMD Ryzen 5 3600 6-Core Processor with 48GB RAM
  \item GPU: GeForce RTX 3090 Ti 24GB
\end{itemize}

\end{document}
